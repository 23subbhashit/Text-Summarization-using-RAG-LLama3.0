{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":64148,"databundleVersionId":7669720,"sourceType":"competition"},{"sourceId":7705679,"sourceType":"datasetVersion","datasetId":4498747},{"sourceId":11382,"sourceType":"modelInstanceVersion","modelInstanceId":8318,"modelId":3301}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><h1>RAG using Gemma, Langchain and ChromaDB</h1></center>\n<center><img src=\"https://res.infoq.com/news/2024/02/google-gemma-open-model/en/headerimage/generatedHeaderImage-1708977571481.jpg\" width=\"400\"></center>\n\n\n# Introduction\n\nThis notebook demonstrates how to build a retrieval augmented generation (RAG) system using Gemma as a large language model (LLM), Langchain for tools to process input files, and ChromaDB as vector database.\n\n## What is RAG?\n\nRetriever augmented generation (RAG) is a system that improves the response generated by a LLM in two ways:\n- First, the information is retrieved from a dataset that is stored in vector database; the query is used to perform similarity search in the documents stored in the vector database.\n- Second, by restraining the context provided to the LLM to content that is similar with the initial query, stored in the vector database, we can reduce significantly (or even eliminate) LLM's halucinations, since the answer is provided from the context of the stored documents.\n\nAn important advantage of this approach is that we do not need to fine-tune the LLM with our custom data; instead, the data is ingested (cleaned, transformed, chunked, and indexed in the vector database).\n\n## Procedure\n\nWe create two classes:\n* AIAgent - An AI Agent that query Gemma LLM using a custom prompt that instruct Gemma to generate and answer (from the query) by refering to the context (as well provided); the answer to the AI Agent query function is then returned.\n* RAGSystem - initialized with the dataset with Data Science information, with an AIAgent object. In the init function of this class, we ingest the data from the dataset in the vector database. This class have as well a query member function. In this function we first perform similarity search with the query to the vector database. Then, we call the generate function of the ai agent object. Before returning the answer, we use a predefined template to compose the overal response from the question, answer and the context retrieved.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Packages instalation and configurations","metadata":{}},{"cell_type":"code","source":"# install required libraries\n!pip install -q -U git+https://github.com/huggingface/transformers.git\n!pip install accelerate\n!pip install -i https://pypi.org/simple/ bitsandbytes\n!pip install langchain\n!pip install sentence-transformers\n!pip install chromadb","metadata":{"execution":{"iopub.status.busy":"2024-12-22T15:24:07.777012Z","iopub.execute_input":"2024-12-22T15:24:07.777301Z","iopub.status.idle":"2024-12-22T15:26:15.711142Z","shell.execute_reply.started":"2024-12-22T15:24:07.777251Z","shell.execute_reply":"2024-12-22T15:26:15.710305Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.28.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.27.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.2)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nLooking in indexes: https://pypi.org/simple/\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl.metadata (2.9 kB)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: typing_extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (4.9.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nDownloading bitsandbytes-0.45.0-py3-none-manylinux_2_24_x86_64.whl (69.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: bitsandbytes\nSuccessfully installed bitsandbytes-0.45.0\nCollecting langchain\n  Downloading langchain-0.3.13-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nCollecting langchain-core<0.4.0,>=0.3.26 (from langchain)\n  Downloading langchain_core-0.3.28-py3-none-any.whl.metadata (6.3 kB)\nCollecting langchain-text-splitters<0.4.0,>=0.3.3 (from langchain)\n  Downloading langchain_text_splitters-0.3.4-py3-none-any.whl.metadata (2.3 kB)\nCollecting langsmith<0.3,>=0.1.17 (from langchain)\n  Downloading langsmith-0.2.4-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.26.4)\nCollecting pydantic<3.0.0,>=2.7.4 (from langchain)\n  Downloading pydantic-2.10.4-py3-none-any.whl.metadata (29 kB)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.31.0)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (1.33)\nCollecting packaging<25,>=23.2 (from langchain-core<0.4.0,>=0.3.26->langchain)\n  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.26->langchain) (4.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.27.0)\nCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.3,>=0.1.17->langchain)\n  Downloading orjson-3.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.8/41.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.3,>=0.1.17->langchain)\n  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\nCollecting pydantic-core==2.27.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n  Downloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nCollecting typing-extensions>=4.7 (from langchain-core<0.4.0,>=0.3.26->langchain)\n  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.2.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.4)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.26->langchain) (2.4)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.0)\nDownloading langchain-0.3.13-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.28-py3-none-any.whl (411 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_text_splitters-0.3.4-py3-none-any.whl (27 kB)\nDownloading langsmith-0.2.4-py3-none-any.whl (320 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic-2.10.4-py3-none-any.whl (431 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.8/431.8 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading orjson-3.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (131 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.3/131.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nInstalling collected packages: typing-extensions, packaging, orjson, requests-toolbelt, pydantic-core, pydantic, langsmith, langchain-core, langchain-text-splitters, langchain\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.9.0\n    Uninstalling typing_extensions-4.9.0:\n      Successfully uninstalled typing_extensions-4.9.0\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n  Attempting uninstall: requests-toolbelt\n    Found existing installation: requests-toolbelt 0.10.1\n    Uninstalling requests-toolbelt-0.10.1:\n      Successfully uninstalled requests-toolbelt-0.10.1\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.14.6\n    Uninstalling pydantic_core-2.14.6:\n      Successfully uninstalled pydantic_core-2.14.6\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 2.5.3\n    Uninstalling pydantic-2.5.3:\n      Successfully uninstalled pydantic-2.5.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\nkeras-cv 0.8.2 requires keras-core, which is not installed.\nkeras-nlp 0.8.2 requires keras-core, which is not installed.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.4.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.2 which is incompatible.\njupyterlab 4.1.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.9.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.3.1 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.3.13 langchain-core-0.3.28 langchain-text-splitters-0.3.4 langsmith-0.2.4 orjson-3.10.12 packaging-24.2 pydantic-2.10.4 pydantic-core-2.16.3 requests-toolbelt-1.0.0 typing-extensions-4.12.2\nCollecting sentence-transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.48.0.dev0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.66.1)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.1.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.20.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.27.0)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (9.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.0)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.12.25)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.2.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-3.3.1\nCollecting chromadb\n  Downloading chromadb-0.5.23-py3-none-any.whl.metadata (6.8 kB)\nCollecting build>=1.0.3 (from chromadb)\n  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.10.4)\nCollecting chroma-hnswlib==0.7.6 (from chromadb)\n  Downloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nRequirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.108.0)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.25.0)\nRequirement already satisfied: numpy>=1.22.5 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.26.4)\nCollecting posthog>=2.4.0 (from chromadb)\n  Downloading posthog-3.7.4-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: typing_extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.12.2)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.50b0-py3-none-any.whl.metadata (2.1 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nCollecting tokenizers<=0.20.3,>=0.13.2 (from chromadb)\n  Downloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.66.1)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (7.4.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.1.1)\nCollecting grpcio>=1.58.0 (from chromadb)\n  Downloading grpcio-1.68.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Downloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.8 kB)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.9.0)\nCollecting kubernetes>=28.1.0 (from chromadb)\n  Downloading kubernetes-31.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (8.2.3)\nRequirement already satisfied: PyYAML>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.0.1)\nRequirement already satisfied: mmh3>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.1.0)\nRequirement already satisfied: orjson>=3.9.12 in /opt/conda/lib/python3.10/site-packages (from chromadb) (3.10.12)\nRequirement already satisfied: httpx>=0.27.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.27.0)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (13.7.0)\nRequirement already satisfied: packaging>=19.1 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (24.2)\nCollecting pyproject_hooks (from build>=1.0.3->chromadb)\n  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.32.0.post1)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (4.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.4)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.6)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.26.1)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.31.0)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\nCollecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb)\n  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\nRequirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\nRequirement already satisfied: backoff<3.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-proto==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\nCollecting opentelemetry-instrumentation-asgi==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.50b0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.50b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.50b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-util-http==0.50b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.50b0-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.50b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.29.0-py3-none-any.whl.metadata (1.4 kB)\nINFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.49b2-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation-asgi==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.49b2-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.49b2-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-util-http==0.49b2 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.49b2-py3-none-any.whl.metadata (2.5 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.28.2-py3-none-any.whl.metadata (1.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.49b1-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation-asgi==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.49b1-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.49b1-py3-none-any.whl.metadata (6.2 kB)\nCollecting opentelemetry-semantic-conventions==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.49b1-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-util-http==0.49b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.49b1-py3-none-any.whl.metadata (2.5 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.28.1-py3-none-any.whl.metadata (1.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.49b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation-asgi==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.49b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.49b0-py3-none-any.whl.metadata (6.2 kB)\nCollecting opentelemetry-semantic-conventions==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.49b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-util-http==0.49b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.49b0-py3-none-any.whl.metadata (2.5 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.28.0-py3-none-any.whl.metadata (1.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.48b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation-asgi==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.48b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.48b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.48b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-util-http==0.48b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.48b0-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.48b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.0.3)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.27.0-py3-none-any.whl.metadata (1.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.47b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation-asgi==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.47b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.47b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.47b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-util-http==0.47b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.47b0-py3-none-any.whl.metadata (2.5 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.26.0-py3-none-any.whl.metadata (1.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.46b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation-asgi==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.46b0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.46b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-util-http==0.46b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.46b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-api>=1.2.0 (from chromadb)\n  Downloading opentelemetry_api-1.25.0-py3-none-any.whl.metadata (1.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.45b0-py3-none-any.whl.metadata (2.0 kB)\nCollecting opentelemetry-instrumentation-asgi==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.45b0-py3-none-any.whl.metadata (1.9 kB)\nCollecting opentelemetry-instrumentation==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.45b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.45b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-util-http==0.45b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.45b0-py3-none-any.whl.metadata (2.4 kB)\nINFO: pip is still looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation-asgi==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-util-http==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl.metadata (2.4 kB)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\nCollecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl.metadata (2.5 kB)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\nCollecting pydantic-core==2.27.2 (from pydantic>=1.9->chromadb)\n  Using cached pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (2.17.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from tokenizers<=0.20.3,>=0.13.2->chromadb) (0.27.0)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<=0.20.3,>=0.13.2->chromadb) (2024.3.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\nDownloading chromadb-0.5.23-py3-none-any.whl (628 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.3/628.3 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bcrypt-4.2.1-cp39-abi3-manylinux_2_28_x86_64.whl (278 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\nDownloading grpcio-1.68.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m82.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading kubernetes-31.0.0-py2.py3-none-any.whl (1.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.20.1-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (13.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m89.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\nDownloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\nDownloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\nDownloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\nDownloading posthog-3.7.4-py2.py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hUsing cached pydantic_core-2.27.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\nDownloading tokenizers-0.20.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading durationpy-0.9-py3-none-any.whl (3.5 kB)\nDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\nDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\nDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53725 sha256=992af2291a569442d1e6e4cfdf8de736665e0739c7deef10b29e2c60085cb9fe\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, monotonic, durationpy, pyproject_hooks, pydantic-core, opentelemetry-util-http, humanfriendly, grpcio, chroma-hnswlib, bcrypt, asgiref, posthog, coloredlogs, build, tokenizers, opentelemetry-instrumentation, onnxruntime, kubernetes, opentelemetry-instrumentation-asgi, opentelemetry-instrumentation-fastapi, chromadb\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.16.3\n    Uninstalling pydantic_core-2.16.3:\n      Successfully uninstalled pydantic_core-2.16.3\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.51.1\n    Uninstalling grpcio-1.51.1:\n      Successfully uninstalled grpcio-1.51.1\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Uninstalling tokenizers-0.21.0:\n      Successfully uninstalled tokenizers-0.21.0\n  Attempting uninstall: kubernetes\n    Found existing installation: kubernetes 26.1.0\n    Uninstalling kubernetes-26.1.0:\n      Successfully uninstalled kubernetes-26.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 24.2 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 31.0.0 which is incompatible.\nkfp 2.5.0 requires requests-toolbelt<1,>=0.8.0, but you have requests-toolbelt 1.0.0 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.0.5 which is incompatible.\ntransformers 4.48.0.dev0 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.20.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asgiref-3.8.1 bcrypt-4.2.1 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-0.5.23 coloredlogs-15.0.1 durationpy-0.9 grpcio-1.60.0 humanfriendly-10.0 kubernetes-31.0.0 monotonic-1.6 onnxruntime-1.20.1 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-util-http-0.43b0 posthog-3.7.4 pydantic-core-2.27.2 pypika-0.48.9 pyproject_hooks-1.2.0 tokenizers-0.20.3\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"! pip install transformers -U\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T15:29:04.094633Z","iopub.execute_input":"2024-12-22T15:29:04.095431Z","iopub.status.idle":"2024-12-22T15:29:13.354436Z","shell.execute_reply.started":"2024-12-22T15:29:04.095400Z","shell.execute_reply":"2024-12-22T15:29:13.353387Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.48.0.dev0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nCollecting tokenizers<0.22,>=0.21 (from transformers)\n  Using cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.3.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nUsing cached tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\nInstalling collected packages: tokenizers\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.20.3\n    Uninstalling tokenizers-0.20.3:\n      Successfully uninstalled tokenizers-0.20.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nchromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.21.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed tokenizers-0.21.0\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"!pip install -U langchain-community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T15:29:44.054858Z","iopub.execute_input":"2024-12-22T15:29:44.055906Z","iopub.status.idle":"2024-12-22T15:29:54.943458Z","shell.execute_reply.started":"2024-12-22T15:29:44.055848Z","shell.execute_reply":"2024-12-22T15:29:54.942571Z"}},"outputs":[{"name":"stdout","text":"Collecting langchain-community\n  Downloading langchain_community-0.3.13-py3-none-any.whl.metadata (2.9 kB)\nRequirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (6.0.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (3.9.1)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.6.4)\nCollecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: langchain<0.4.0,>=0.3.13 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.3.13)\nRequirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.3.28)\nRequirement already satisfied: langsmith<0.3,>=0.1.125 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.2.4)\nRequirement already satisfied: numpy<2,>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (1.26.4)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n  Downloading pydantic_settings-2.7.0-py3-none-any.whl.metadata (3.5 kB)\nRequirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.31.0)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (8.2.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\nRequirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /opt/conda/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.13->langchain-community) (0.3.4)\nRequirement already satisfied: pydantic<3.0.0,>=2.7.4 in /opt/conda/lib/python3.10/site-packages (from langchain<0.4.0,>=0.3.13->langchain-community) (2.10.4)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.4.0,>=0.3.27->langchain-community) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (0.27.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.3,>=0.1.125->langchain-community) (1.0.0)\nRequirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (4.2.0)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.0.4)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain-community) (2.4)\nRequirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.13->langchain-community) (0.6.0)\nRequirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.13->langchain-community) (2.27.2)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-community) (1.2.0)\nDownloading langchain_community-0.3.13-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading pydantic_settings-2.7.0-py3-none-any.whl (29 kB)\nInstalling collected packages: httpx-sse, pydantic-settings, langchain-community\nSuccessfully installed httpx-sse-0.4.0 langchain-community-0.3.13 pydantic-settings-2.7.0\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom langchain.document_loaders import CSVLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain.vectorstores import Chroma\n\nfrom IPython.display import display, Markdown\n","metadata":{"execution":{"iopub.status.busy":"2024-12-22T15:29:54.945675Z","iopub.execute_input":"2024-12-22T15:29:54.946134Z","iopub.status.idle":"2024-12-22T15:29:54.995885Z","shell.execute_reply.started":"2024-12-22T15:29:54.946100Z","shell.execute_reply":"2024-12-22T15:29:54.995053Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# AI Agent class","metadata":{}},{"cell_type":"code","source":"class AIAgent:\n    \"\"\"\n    Gemma 2b-it assistant.\n    It uses Gemma transformers 2b-it/2.\n    \"\"\"\n    def __init__(self, max_length=256):\n        self.max_length = max_length\n        self.tokenizer = AutoTokenizer.from_pretrained(\"/kaggle/input/gemma/transformers/2b-it/2\")\n        self.gemma_lm = AutoModelForCausalLM.from_pretrained(\"/kaggle/input/gemma/transformers/2b-it/2\")\n\n    def create_prompt(self, query, context):\n        # prompt template\n        prompt = f\"\"\"\n        You are an AI Agent specialized to answer to questions about Data Science.\n        Explain the concept or answer the question about Data Science.\n        In order to create the answer, please only use the information from the\n        context provided (Context). Do not include other information.\n        Answer with simple words.\n        If needed, include also explanations.\n        Question: {query}\n        Context: {context}\n        Answer:\n        \"\"\"\n        return prompt\n    \n    def generate(self, query, retrieved_info):\n        prompt = self.create_prompt(query, retrieved_info)\n        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids\n        # Answer generation\n        answer = self.gemma_lm.generate(\n            input_ids,\n            #max_length=self.max_length, # limit the answer to max_length\n            max_new_tokens=self.max_length\n        )\n        # Decode and return the answer\n        answer = self.tokenizer.decode(answer[0], skip_special_tokens=True, skip_prompt=True)\n        return prompt, answer","metadata":{"execution":{"iopub.status.busy":"2024-12-22T15:29:58.945532Z","iopub.execute_input":"2024-12-22T15:29:58.946119Z","iopub.status.idle":"2024-12-22T15:29:58.952796Z","shell.execute_reply.started":"2024-12-22T15:29:58.946090Z","shell.execute_reply":"2024-12-22T15:29:58.951938Z"},"trusted":true},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"## Test the AIAgent","metadata":{}},{"cell_type":"code","source":"ai_agent = AIAgent()","metadata":{"execution":{"iopub.status.busy":"2024-12-22T15:30:10.734589Z","iopub.execute_input":"2024-12-22T15:30:10.734936Z","iopub.status.idle":"2024-12-22T15:31:05.674841Z","shell.execute_reply.started":"2024-12-22T15:30:10.734910Z","shell.execute_reply":"2024-12-22T15:31:05.674018Z"},"trusted":true},"outputs":[{"name":"stderr","text":"2024-12-22 15:30:13.910611: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-12-22 15:30:13.910728: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-12-22 15:30:14.030064: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"553bb9a6e7e54877977ef9305558801a"}},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"Let's use the context from the Data Science interview Q&A treasury.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\npd.set_option('display.max_colwidth', 1000)\ndata_df = pd.read_csv(\"/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv\")\ndata_df.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-12-22T15:31:05.677668Z","iopub.execute_input":"2024-12-22T15:31:05.678628Z","iopub.status.idle":"2024-12-22T15:31:05.699768Z","shell.execute_reply.started":"2024-12-22T15:31:05.678603Z","shell.execute_reply":"2024-12-22T15:31:05.699056Z"},"trusted":true},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                                                        question  \\\n0                                         What is supervised machine learning? 👶   \n1  What is regression? Which models can you use to solve a regression problem? 👶   \n2                                What is linear regression? When do we use it? 👶   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    answer  \n0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  Supervised learning is a type of machine learning in which our algorithms are trained using well-labeled training data, and machines predict the output based on that data. Labeled data indicates that the input data has already been tagged with the appropriate output. Basically, it is the task of learning a function that maps the input set and returns an output. Some of its examples are: Linear Regression, Logistic Regression, KNN, etc.  \n1  Regression is a part of supervised ML. Regression models investigate the relationship between a dependent (target) and independent variable (s) (predictor).\\nHere are some common regression models\\n\\n- *Linear Regression* establishes a linear relationship between target and predictor (s). It predicts a numeric value and has a shape of a straight line.\\n- *Polynomial Regression* has a regression equation with the power of independent variable more than 1. It is a curve that fits into the data points.\\n- *Ridge Regression* helps when predictors are highly correlated (multicollinearity problem). It penalizes the squares of regression coefficients but doesn’t allow the coefficients to reach zeros (uses L2 regularization).\\n- *Lasso Regression* penalizes the absolute values of regression coefficients and allows some of the coefficients to reach absolute zero (thereby allowing feature selection).  \n2                                                                                                                                                                                                                                                                                                 Linear regression is a model that assumes a linear relationship between the input variables (X) and the single output variable (y).\\n\\nWith a simple equation:\\n\\n```\\ny = B0 + B1*x1 + ... + Bn * xN\\n```\\n\\nB is regression coefficients, x values are the independent (explanatory) variables  and y is dependent variable.\\n\\nThe case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.\\n\\nSimple linear regression:\\n\\n```\\ny = B0 + B1*x1\\n```\\n\\nMultiple linear regression:\\n\\n```\\ny = B0 + B1*x1 + ... + Bn * xN\\n```  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>What is supervised machine learning? 👶</td>\n      <td>Supervised learning is a type of machine learning in which our algorithms are trained using well-labeled training data, and machines predict the output based on that data. Labeled data indicates that the input data has already been tagged with the appropriate output. Basically, it is the task of learning a function that maps the input set and returns an output. Some of its examples are: Linear Regression, Logistic Regression, KNN, etc.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>What is regression? Which models can you use to solve a regression problem? 👶</td>\n      <td>Regression is a part of supervised ML. Regression models investigate the relationship between a dependent (target) and independent variable (s) (predictor).\\nHere are some common regression models\\n\\n- *Linear Regression* establishes a linear relationship between target and predictor (s). It predicts a numeric value and has a shape of a straight line.\\n- *Polynomial Regression* has a regression equation with the power of independent variable more than 1. It is a curve that fits into the data points.\\n- *Ridge Regression* helps when predictors are highly correlated (multicollinearity problem). It penalizes the squares of regression coefficients but doesn’t allow the coefficients to reach zeros (uses L2 regularization).\\n- *Lasso Regression* penalizes the absolute values of regression coefficients and allows some of the coefficients to reach absolute zero (thereby allowing feature selection).</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>What is linear regression? When do we use it? 👶</td>\n      <td>Linear regression is a model that assumes a linear relationship between the input variables (X) and the single output variable (y).\\n\\nWith a simple equation:\\n\\n```\\ny = B0 + B1*x1 + ... + Bn * xN\\n```\\n\\nB is regression coefficients, x values are the independent (explanatory) variables  and y is dependent variable.\\n\\nThe case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.\\n\\nSimple linear regression:\\n\\n```\\ny = B0 + B1*x1\\n```\\n\\nMultiple linear regression:\\n\\n```\\ny = B0 + B1*x1 + ... + Bn * xN\\n```</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"context = data_df.iloc[0].answer\nprint(\"Context: \", context)\nprompt, answer = ai_agent.generate(query=\"What is supervised learning?\", retrieved_info=context)\nprint(\"LLM Answer: \", answer)","metadata":{"execution":{"iopub.status.busy":"2024-12-22T15:31:05.700695Z","iopub.execute_input":"2024-12-22T15:31:05.700979Z","iopub.status.idle":"2024-12-22T15:31:38.427693Z","shell.execute_reply.started":"2024-12-22T15:31:05.700957Z","shell.execute_reply":"2024-12-22T15:31:38.426647Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Context:  Supervised learning is a type of machine learning in which our algorithms are trained using well-labeled training data, and machines predict the output based on that data. Labeled data indicates that the input data has already been tagged with the appropriate output. Basically, it is the task of learning a function that maps the input set and returns an output. Some of its examples are: Linear Regression, Logistic Regression, KNN, etc.\nLLM Answer:  \n        You are an AI Agent specialized to answer to questions about Data Science.\n        Explain the concept or answer the question about Data Science.\n        In order to create the answer, please only use the information from the\n        context provided (Context). Do not include other information.\n        Answer with simple words.\n        If needed, include also explanations.\n        Question: What is supervised learning?\n        Context: Supervised learning is a type of machine learning in which our algorithms are trained using well-labeled training data, and machines predict the output based on that data. Labeled data indicates that the input data has already been tagged with the appropriate output. Basically, it is the task of learning a function that maps the input set and returns an output. Some of its examples are: Linear Regression, Logistic Regression, KNN, etc.\n        Answer:\n        Sure, here's a simple explanation of supervised learning:\n\nSupervised learning is a type of machine learning where algorithms are trained using well-labeled training data. This means that the input data has already been tagged with the appropriate output. The task is to learn a function that maps the input set and returns an output.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"class RAGSystem:\n    \"\"\"Sentence embedding based Retrieval Based Augmented generation.\n        Given database of pdf files, retriever finds num_retrieved_docs relevant documents\"\"\"\n    def __init__(self, ai_agent, num_retrieved_docs=2):\n        # load the data\n        self.num_docs = num_retrieved_docs\n        self.ai_agent = ai_agent\n        loader = CSVLoader(\"/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv\")\n        documents = loader.load()\n        self.template = \"\\n\\nQuestion:\\n{question}\\n\\nPrompt:\\n{prompt}\\n\\nAnswer:\\n{answer}\\n\\nContext:\\n{context}\"\n        \n        text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=800, \n            chunk_overlap=100)\n        all_splits = text_splitter.split_documents(documents)\n        # create a vectorstore database\n        embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n        self.vector_db = Chroma.from_documents(documents=all_splits, \n                                               embedding=embeddings, \n                                               persist_directory=\"chroma_db\")\n        self.retriever = self.vector_db.as_retriever()\n\n    def retrieve(self, query):\n        # retrieve top k similar documents to query\n        docs = self.retriever.get_relevant_documents(query)\n        return docs\n    \n    def query(self, query):\n        # generate the answer\n        context = self.retrieve(query)\n        data = \"\"\n        for item in list(context):\n            data += item.page_content\n            \n        data = data[:500]\n\n        prompt, answer = self.ai_agent.generate(query, data)\n        \n        return self.template.format(question=query,\n                                    prompt=prompt,\n                                   answer=answer,\n                                   context=context)\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-12-22T15:32:09.464936Z","iopub.execute_input":"2024-12-22T15:32:09.465755Z","iopub.status.idle":"2024-12-22T15:32:09.472794Z","shell.execute_reply.started":"2024-12-22T15:32:09.465728Z","shell.execute_reply":"2024-12-22T15:32:09.471883Z"},"trusted":true},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def colorize_text(text):\n    for word, color in zip([\"Question\", \"Prompt\", \"Answer\", \"Context\"], [\"blue\", \"magenta\", \"red\", \"green\"]):\n        text = text.replace(f\"\\n\\n{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-12-22T15:32:09.784059Z","iopub.execute_input":"2024-12-22T15:32:09.784346Z","iopub.status.idle":"2024-12-22T15:32:09.788926Z","shell.execute_reply.started":"2024-12-22T15:32:09.784326Z","shell.execute_reply":"2024-12-22T15:32:09.787966Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"# Test the RAG system","metadata":{}},{"cell_type":"code","source":"rag_system = RAGSystem(ai_agent)","metadata":{"execution":{"iopub.status.busy":"2024-12-22T15:32:11.779743Z","iopub.execute_input":"2024-12-22T15:32:11.780075Z","iopub.status.idle":"2024-12-22T15:32:18.688350Z","shell.execute_reply.started":"2024-12-22T15:32:11.780051Z","shell.execute_reply":"2024-12-22T15:32:18.687646Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/197962237.py:17: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5291ce2c5e214b1cbd851c5047fffc04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f1e718ead914d86bf29a4ccd7f09ee2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc82c7d9badf41d8b81c71b83fd90e95"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bdbbe5fa3b242fdb7122302fec50b4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"509358c06356409c9277de70580fc255"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4df79399517e4d0bad99bf61a3833c24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"793383881964487387cb32c5b2fca456"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c490517d5a5241f8af4f649eb4dd3be3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8033054729f4496db05ef187445b4158"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07a9b96ac50b4d768770f1fda3cf4741"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98cb2ea2047149db916b7f34bb8a41b2"}},"metadata":{}}],"execution_count":20},{"cell_type":"markdown","source":"Let's try first with few of the questions from the data we used for the retrieval system.","metadata":{}},{"cell_type":"code","source":"answer = rag_system.query(data_df.iloc[0].question)\ndisplay(Markdown(colorize_text(answer)))","metadata":{"execution":{"iopub.status.busy":"2024-12-22T15:32:25.854466Z","iopub.execute_input":"2024-12-22T15:32:25.855383Z","iopub.status.idle":"2024-12-22T15:32:39.623751Z","shell.execute_reply.started":"2024-12-22T15:32:25.855354Z","shell.execute_reply":"2024-12-22T15:32:39.622844Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/197962237.py:25: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  docs = self.retriever.get_relevant_documents(query)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='blue'>Question:</font>**\nWhat is supervised machine learning? 👶\n\n**<font color='magenta'>Prompt:</font>**\n\n        You are an AI Agent specialized to answer to questions about Data Science.\n        Explain the concept or answer the question about Data Science.\n        In order to create the answer, please only use the information from the\n        context provided (Context). Do not include other information.\n        Answer with simple words.\n        If needed, include also explanations.\n        Question: What is supervised machine learning? 👶\n        Context: question: What is supervised machine learning? 👶\nanswer: Supervised learning is a type of machine learning in which our algorithms are trained using well-labeled training data, and machines predict the output based on that data. Labeled data indicates that the input data has already been tagged with the appropriate output. Basically, it is the task of learning a function that maps the input set and returns an output. Some of its examples are: Linear Regression, Logistic Regression, KNN, etc.ques\n        Answer:\n        \n\n**<font color='red'>Answer:</font>**\n\n        You are an AI Agent specialized to answer to questions about Data Science.\n        Explain the concept or answer the question about Data Science.\n        In order to create the answer, please only use the information from the\n        context provided (Context). Do not include other information.\n        Answer with simple words.\n        If needed, include also explanations.\n        Question: What is supervised machine learning? 👶\n        Context: question: What is supervised machine learning? 👶\nanswer: Supervised learning is a type of machine learning in which our algorithms are trained using well-labeled training data, and machines predict the output based on that data. Labeled data indicates that the input data has already been tagged with the appropriate output. Basically, it is the task of learning a function that maps the input set and returns an output. Some of its examples are: Linear Regression, Logistic Regression, KNN, etc.ques\n        Answer:\n        Supervised machine learning is a type of machine learning where algorithms are trained using well-labeled training data.\n\n**<font color='green'>Context:</font>**\n[Document(metadata={'row': 0, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: What is supervised machine learning? 👶\\nanswer: Supervised learning is a type of machine learning in which our algorithms are trained using well-labeled training data, and machines predict the output based on that data. Labeled data indicates that the\\xa0input data has already been tagged with the appropriate output. Basically, it is the task of learning a function that maps the input set and returns an output. Some of its examples are: Linear Regression, Logistic Regression, KNN, etc.'), Document(metadata={'row': 147, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: How can we use machine learning for search? \\u200d⭐️\\nanswer: Answer here'), Document(metadata={'row': 132, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: What is unsupervised learning? 👶\\nanswer: Unsupervised learning aims to detect patterns in data where no labels are given.'), Document(metadata={'row': 20, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: What is classification? Which models would you use to solve a classification problem? 👶\\nanswer: Classification problems are problems in which our prediction space is discrete, i.e. there is a finite number of values the output variable can be. Some models which can be used to solve classification problems are: logistic regression, decision tree, random forests, multi-layer perceptron, one-vs-all, amongst others.')]"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"answer = rag_system.query(data_df.iloc[3].question)\ndisplay(Markdown(colorize_text(answer)))","metadata":{"execution":{"iopub.status.busy":"2024-12-22T15:32:59.023678Z","iopub.execute_input":"2024-12-22T15:32:59.024310Z","iopub.status.idle":"2024-12-22T15:33:20.331836Z","shell.execute_reply.started":"2024-12-22T15:32:59.024280Z","shell.execute_reply":"2024-12-22T15:33:20.330970Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='blue'>Question:</font>**\nWhat are the main assumptions of linear regression? ⭐\n\n**<font color='magenta'>Prompt:</font>**\n\n        You are an AI Agent specialized to answer to questions about Data Science.\n        Explain the concept or answer the question about Data Science.\n        In order to create the answer, please only use the information from the\n        context provided (Context). Do not include other information.\n        Answer with simple words.\n        If needed, include also explanations.\n        Question: What are the main assumptions of linear regression? ⭐\n        Context: question: What are the main assumptions of linear regression? ⭐\nanswer: There are several assumptions of linear regression. If any of them is violated, model predictions and interpretation may be worthless or misleading.question: What is linear regression? When do we use it? 👶\nanswer: Linear regression is a model that assumes a linear relationship between the input variables (X) and the single output variable (y).\n\nWith a simple equation:\n\n```\ny = B0 + B1*x1 + ... + Bn * xN\n```\n\nB is regression \n        Answer:\n        \n\n**<font color='red'>Answer:</font>**\n\n        You are an AI Agent specialized to answer to questions about Data Science.\n        Explain the concept or answer the question about Data Science.\n        In order to create the answer, please only use the information from the\n        context provided (Context). Do not include other information.\n        Answer with simple words.\n        If needed, include also explanations.\n        Question: What are the main assumptions of linear regression? ⭐\n        Context: question: What are the main assumptions of linear regression? ⭐\nanswer: There are several assumptions of linear regression. If any of them is violated, model predictions and interpretation may be worthless or misleading.question: What is linear regression? When do we use it? 👶\nanswer: Linear regression is a model that assumes a linear relationship between the input variables (X) and the single output variable (y).\n\nWith a simple equation:\n\n```\ny = B0 + B1*x1 + ... + Bn * xN\n```\n\nB is regression \n        Answer:\n        Linear regression is used when we have a dependent variable that is linearly related to one or more independent variables. It is commonly used in various fields, including business, science, and social sciences.\n\n**<font color='green'>Context:</font>**\n[Document(metadata={'row': 3, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: What are the main assumptions of linear regression? ⭐\\nanswer: There are several assumptions of linear regression. If any of them is violated, model predictions and interpretation may be worthless or misleading.'), Document(metadata={'row': 2, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: What is linear regression? When do we use it? 👶\\nanswer: Linear regression is a model that assumes a linear relationship between the input variables (X) and the single output variable (y).\\n\\nWith a simple equation:\\n\\n```\\ny = B0 + B1*x1 + ... + Bn * xN\\n```\\n\\nB is regression coefficients, x values are the independent (explanatory) variables  and y is dependent variable.\\n\\nThe case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression.\\n\\nSimple linear regression:\\n\\n```\\ny = B0 + B1*x1\\n```\\n\\nMultiple linear regression:\\n\\n```\\ny = B0 + B1*x1 + ... + Bn * xN\\n```'), Document(metadata={'row': 39, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: What happens to our linear regression model if we have three columns in our data: x, y, z \\u200a—\\u200a and z is a sum of x and y? \\u200d⭐️\\nanswer: We would not be able to perform the regression. Because z is linearly dependent on x and y so when performing the regression <img src=\"https://render.githubusercontent.com/render/math?math={X}^{T}{X}\"> would be a singular (not invertible) matrix.'), Document(metadata={'row': 1, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: What is regression? Which models can you use to solve a regression problem? 👶\\nanswer: Regression is a part of supervised ML. Regression models investigate the relationship between a dependent (target) and independent variable (s) (predictor).\\nHere are some common regression models')]"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"answer = rag_system.query(\"What’s the normal distribution? Why do we care about it?\")\ndisplay(Markdown(colorize_text(answer)))","metadata":{"execution":{"iopub.status.busy":"2024-12-22T15:33:20.333722Z","iopub.execute_input":"2024-12-22T15:33:20.334346Z","iopub.status.idle":"2024-12-22T15:35:10.624923Z","shell.execute_reply.started":"2024-12-22T15:33:20.334318Z","shell.execute_reply":"2024-12-22T15:35:10.624061Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='blue'>Question:</font>**\nWhat’s the normal distribution? Why do we care about it?\n\n**<font color='magenta'>Prompt:</font>**\n\n        You are an AI Agent specialized to answer to questions about Data Science.\n        Explain the concept or answer the question about Data Science.\n        In order to create the answer, please only use the information from the\n        context provided (Context). Do not include other information.\n        Answer with simple words.\n        If needed, include also explanations.\n        Question: What’s the normal distribution? Why do we care about it?\n        Context: question: What’s the normal distribution? Why do we care about it? 👶\nanswer: The normal distribution is a continuous probability distribution whose probability density function takes the following formula:\n\n![formula](https://mathworld.wolfram.com/images/equations/NormalDistribution/NumberedEquation1.gif)\n\nwhere μ is the mean and σ is the standard deviation of the distribution.\n\nThe normal distribution derives its importance from the **Central Limit Theorem**, which states that if we draw a larg\n        Answer:\n        \n\n**<font color='red'>Answer:</font>**\n\n        You are an AI Agent specialized to answer to questions about Data Science.\n        Explain the concept or answer the question about Data Science.\n        In order to create the answer, please only use the information from the\n        context provided (Context). Do not include other information.\n        Answer with simple words.\n        If needed, include also explanations.\n        Question: What’s the normal distribution? Why do we care about it?\n        Context: question: What’s the normal distribution? Why do we care about it? 👶\nanswer: The normal distribution is a continuous probability distribution whose probability density function takes the following formula:\n\n![formula](https://mathworld.wolfram.com/images/equations/NormalDistribution/NumberedEquation1.gif)\n\nwhere μ is the mean and σ is the standard deviation of the distribution.\n\nThe normal distribution derives its importance from the **Central Limit Theorem**, which states that if we draw a larg\n        Answer:\n        Sure, here's an explanation of the normal distribution:\n\nThe normal distribution is a bell-shaped curve that is commonly used to model real-world phenomena. It is often used in statistics, probability theory, and machine learning to model continuous data.\n\nThe normal distribution is important because it has a number of properties that make it a useful tool for modeling real-world data. These properties include:\n\n* The mean and standard deviation of the normal distribution are the same for any set of data. This means that the mean and standard deviation can be used to represent the entire distribution of data.\n* The normal distribution is symmetric, meaning that it is symmetrical about its mean. This means that the probability density is the same on both sides of the mean.\n* The normal distribution is bell-shaped, meaning that it is symmetric about its mean. This means that the probability density is higher in the center of the distribution and lower on the sides.\n\nThese properties make the normal distribution a useful tool for modeling real-world data. It can be used to fit a wide variety of data sets, and it can provide valuable insights into the underlying structure of the data.\n\n**<font color='green'>Context:</font>**\n[Document(metadata={'row': 4, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: What’s the normal distribution? Why do we care about it? 👶\\nanswer: The normal distribution is a continuous probability distribution whose probability density function takes the following formula:\\n\\n![formula](https://mathworld.wolfram.com/images/equations/NormalDistribution/NumberedEquation1.gif)\\n\\nwhere μ is the mean and σ is the standard deviation of the distribution.\\n\\nThe normal distribution derives its importance from the **Central Limit Theorem**, which states that if we draw a large enough number of samples, their mean will follow a normal distribution regardless of the initial distribution of the sample, i.e **the distribution of the mean of the samples is normal**. It is important that each sample is independent from the other.'), Document(metadata={'row': 4, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='This is powerful because it helps us study processes whose population distribution is unknown to us.'), Document(metadata={'row': 5, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: How do we check if a variable follows the normal distribution? \\u200d⭐️\\nanswer: 1. Plot a histogram out of the sampled data. If you can fit the bell-shaped \"normal\" curve to the histogram, then the hypothesis that the underlying random variable follows the normal distribution can not be rejected.\\n2. Check Skewness and Kurtosis of the sampled data. Skewness = 0 and kurtosis = 3 are typical for a normal distribution, so the farther away they are from these values, the more non-normal the distribution.\\n3. Use Kolmogorov-Smirnov or/and Shapiro-Wilk tests for normality. They take into account both Skewness and Kurtosis simultaneously.'), Document(metadata={'row': 9, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: What is the normal equation? \\u200d⭐️\\nanswer: Normal equations are equations obtained by setting equal to zero the partial derivatives of the sum of squared errors (least squares); normal equations allow one to estimate the parameters of a multiple linear regression.')]"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"Let's try also with some \"fresh\" questions.","metadata":{}},{"cell_type":"code","source":"answer = rag_system.query(\"Please explain bias and variance?\")\ndisplay(Markdown(colorize_text(answer)))","metadata":{"execution":{"iopub.status.busy":"2024-12-22T15:35:10.626027Z","iopub.execute_input":"2024-12-22T15:35:10.626275Z","iopub.status.idle":"2024-12-22T15:36:44.016819Z","shell.execute_reply.started":"2024-12-22T15:35:10.626254Z","shell.execute_reply":"2024-12-22T15:36:44.015796Z"},"trusted":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='blue'>Question:</font>**\nPlease explain bias and variance?\n\n**<font color='magenta'>Prompt:</font>**\n\n        You are an AI Agent specialized to answer to questions about Data Science.\n        Explain the concept or answer the question about Data Science.\n        In order to create the answer, please only use the information from the\n        context provided (Context). Do not include other information.\n        Answer with simple words.\n        If needed, include also explanations.\n        Question: Please explain bias and variance?\n        Context: question: What’s the interpretation of the bias term in linear models? ‍⭐️\nanswer: Bias is simply, a difference between predicted value and actual/true value. It can be interpreted as the distance from the average prediction and true value i.e. true value minus mean(predictions). But dont get confused between accuracy and bias.question: What is the bias-variance trade-off? 👶\nanswer: **Bias** is the error introduced by approximating the true underlying function, which can be quite complex, by a s\n        Answer:\n        \n\n**<font color='red'>Answer:</font>**\n\n        You are an AI Agent specialized to answer to questions about Data Science.\n        Explain the concept or answer the question about Data Science.\n        In order to create the answer, please only use the information from the\n        context provided (Context). Do not include other information.\n        Answer with simple words.\n        If needed, include also explanations.\n        Question: Please explain bias and variance?\n        Context: question: What’s the interpretation of the bias term in linear models? ‍⭐️\nanswer: Bias is simply, a difference between predicted value and actual/true value. It can be interpreted as the distance from the average prediction and true value i.e. true value minus mean(predictions). But dont get confused between accuracy and bias.question: What is the bias-variance trade-off? 👶\nanswer: **Bias** is the error introduced by approximating the true underlying function, which can be quite complex, by a s\n        Answer:\n        Sure. Here's a breakdown of bias and variance:\n\n**Bias:** Bias is the difference between the predicted value and the actual/true value. It can be interpreted as the distance from the average prediction and true value.\n\n**Variance:** Variance is a measure of how much the predicted value varies from one sample to another. It can be interpreted as the average of the squared differences between the predicted value and the actual/true value.\n\nThe bias-variance trade-off is a trade-off between bias and variance. The goal of any machine learning algorithm is to achieve a good balance between bias and variance. A low bias means that the model is too simple and does not capture the underlying patterns in the data. A high bias means that the model is too complex and overfits the data. A high variance means that the model is more sensitive to noise in the data. A low variance means that the model is very robust and does not change much with noise in the data.\n\n**<font color='green'>Context:</font>**\n[Document(metadata={'row': 50, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: What’s the interpretation of the bias term in linear models? \\u200d⭐️\\nanswer: Bias is simply, a difference between predicted value and actual/true value. It can be interpreted as the distance from the average prediction and true value i.e. true value minus mean(predictions). But dont get confused between accuracy and bias.'), Document(metadata={'row': 13, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: What is the bias-variance trade-off? 👶\\nanswer: **Bias** is the error introduced by approximating the true underlying function, which can be quite complex, by a simpler model. **Variance** is a model sensitivity to changes in the training dataset.\\n\\n**Bias-variance trade-off** is a relationship between the expected test error and the variance and the bias - both contribute to the level of the test error and ideally should be as small as possible:\\n\\n```\\nExpectedTestError = Variance + Bias² + IrreducibleError\\n```\\n\\nBut as a model complexity increases, the bias decreases and the variance increases which leads to *overfitting*. And vice versa, model simplification helps to decrease the variance but it increases the bias which leads to *underfitting*.'), Document(metadata={'row': 4, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: What’s the normal distribution? Why do we care about it? 👶\\nanswer: The normal distribution is a continuous probability distribution whose probability density function takes the following formula:\\n\\n![formula](https://mathworld.wolfram.com/images/equations/NormalDistribution/NumberedEquation1.gif)\\n\\nwhere μ is the mean and σ is the standard deviation of the distribution.\\n\\nThe normal distribution derives its importance from the **Central Limit Theorem**, which states that if we draw a large enough number of samples, their mean will follow a normal distribution regardless of the initial distribution of the sample, i.e **the distribution of the mean of the samples is normal**. It is important that each sample is independent from the other.'), Document(metadata={'row': 89, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='Assigning random values to weights is better than just 0 assignment. \\n* a) If weights are initialized with very high values the term np.dot(W,X)+b becomes significantly higher and if an activation function like sigmoid() is applied, the function maps its value near to 1 where the slope of gradient changes slowly and learning takes a lot of time.\\n* b) If weights are initialized with low values it gets mapped to 0, where the case is the same as above. This problem is often referred to as the vanishing gradient.')]"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"answer = rag_system.query(\"What is a Dropout?\")\ndisplay(Markdown(colorize_text(answer)))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-22T15:36:44.018454Z","iopub.execute_input":"2024-12-22T15:36:44.018716Z","iopub.status.idle":"2024-12-22T15:38:42.739842Z","shell.execute_reply.started":"2024-12-22T15:36:44.018695Z","shell.execute_reply":"2024-12-22T15:38:42.738951Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Markdown object>","text/markdown":"\n\n**<font color='blue'>Question:</font>**\nWhat is a Dropout?\n\n**<font color='magenta'>Prompt:</font>**\n\n        You are an AI Agent specialized to answer to questions about Data Science.\n        Explain the concept or answer the question about Data Science.\n        In order to create the answer, please only use the information from the\n        context provided (Context). Do not include other information.\n        Answer with simple words.\n        If needed, include also explanations.\n        Question: What is a Dropout?\n        Context: question: What is dropout? Why is it useful? How does it work? ‍⭐️\nanswer: Dropout is a technique that at each training step turns off each neuron with a certain probability of *p*. This way at each iteration we train only *1-p* of neurons, which forces the network not to rely only on the subset of neurons for feature representation. This leads to regularizing effects that are controlled by the hyperparameter *p*.question: What’s pooling in CNN? Why do we need it? ‍⭐️\nanswer: Pooling is a techni\n        Answer:\n        \n\n**<font color='red'>Answer:</font>**\n\n        You are an AI Agent specialized to answer to questions about Data Science.\n        Explain the concept or answer the question about Data Science.\n        In order to create the answer, please only use the information from the\n        context provided (Context). Do not include other information.\n        Answer with simple words.\n        If needed, include also explanations.\n        Question: What is a Dropout?\n        Context: question: What is dropout? Why is it useful? How does it work? ‍⭐️\nanswer: Dropout is a technique that at each training step turns off each neuron with a certain probability of *p*. This way at each iteration we train only *1-p* of neurons, which forces the network not to rely only on the subset of neurons for feature representation. This leads to regularizing effects that are controlled by the hyperparameter *p*.question: What’s pooling in CNN? Why do we need it? ‍⭐️\nanswer: Pooling is a techni\n        Answer:\n        Sure, here's a simplified explanation of the concept of Dropout and its importance in Data Science:\n\n**Dropout**:\n\n* Dropout is a technique used in machine learning algorithms to prevent overfitting by randomly dropping out neurons during training.\n* It involves setting a probability, usually *p*, for each neuron to be dropped out at each training step.\n* This means that only *1-p* neurons are trained at any given iteration.\n* By doing this, the network is forced to rely on a different subset of neurons for feature representation, which helps to reduce overfitting.\n\n**Importance of Dropout**:\n\n* Dropout is important because it helps to:\n    * Reduce overfitting by preventing the network from over-learning from the training data.\n    * Improve generalization performance by forcing the network to learn from a more diverse set of features.\n    * Control the complexity of the model by adjusting the value of *p*.\n\n**Pooling in CNN**:\n\n* Pooling is a technique used in convolutional neural networks (CNNs) to reduce the dimensionality of feature maps by taking a subset of the input features.\n* It involves computing a summary statistic (mean, median, or min-max) of the selected features.\n* Pooling\n\n**<font color='green'>Context:</font>**\n[Document(metadata={'row': 92, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: What is dropout? Why is it useful? How does it work? \\u200d⭐️\\nanswer: Dropout is a technique that at each training step turns off each neuron with a certain probability of *p*. This way at each iteration we train only *1-p* of neurons, which forces the network not to rely only on the subset of neurons for feature representation. This leads to regularizing effects that are controlled by the hyperparameter *p*.'), Document(metadata={'row': 108, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: What’s pooling in CNN? Why do we need it? \\u200d⭐️\\nanswer: Pooling is a technique to downsample the feature map. It allows layers which receive relatively undistorted versions of the input to learn low level features such as lines, while layers deeper in the model can learn more abstract features such as texture.'), Document(metadata={'row': 21, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: What is logistic regression? When do we need to use it? 👶\\nanswer: Logistic regression is a Machine Learning algorithm that is used for binary classification. You should use logistic regression when your Y variable takes only two values, e.g. True and False, \"spam\" and \"not spam\", \"churn\" and \"not churn\" and so on. The variable is said to be a \"binary\" or \"dichotomous\".'), Document(metadata={'row': 54, 'source': '/kaggle/input/data-science-interview-q-and-a-treasury/dataset.csv'}, page_content='question: What is feature selection? Why do we need it? 👶\\nanswer: Feature Selection is a method used to select the relevant features for the model to train on. We need feature selection to remove the irrelevant features which leads the model to under-perform.')]"},"metadata":{}}],"execution_count":25},{"cell_type":"markdown","source":"# Conclusions\n\nWe tested a RAG system developed with Gemma as LLM, Langchain for data loaders utilities, and ChromaDB as database. \nThe RAG system is initialized with a dataset, that is used to populate the vector database, and with an AI Agent, that will query Gemma, given the initial query and the retrieved context.\nTo verify that the result is composed based on the context provided, we include as well the context in the exported result.\n","metadata":{}}]}